# Global defaults (algorithm-agnostic)
global:
  seed: 41
  max_episodes: 1000
  log_interval: 10

# Experiment settings
experiment:
  num_runs: 5
  save_checkpoints: true
  save_figures: true 

# Figure settings
figures:
  # Learning curves - most important for RL
  learning_curves:
    enabled: true
    metrics: ["episode_reward", "episode_length", "loss"]
    smoothing_window: 10
    show_std: true
    
  # Performance comparison across algorithms
  algorithm_comparison:
    enabled: true
    metrics: ["final_performance", "convergence_speed"]
    confidence_interval: 0.95
    
  # Hyperparameter sensitivity (for sweeps)
  hyperparameter_analysis:
    enabled: true
    parameters: ["learning_rate", "epsilon", "gamma"]
    
  # Training stability
  training_stability:
    enabled: true
    metrics: ["reward_variance", "loss_variance"]
    
  # Environment-specific visualizations
  environment_plots:
    enabled: true
    cartpole: ["pole_angle", "cart_position"]
    mountaincar: ["car_position", "car_velocity"]
    
  # Figure export settings
  export:
    format: "png"
    dpi: 300
    save_path: "results/figures/"

# Environment defaults (applied before env-specific overrides)
environment: 
  render_mode: null
  auto_reset: true

  #wrapper config defaults
  wrappers:
    order: ["normalize_observations", "frame_stack", "normalize_rewards"]
    normalize_observations:
      enabled: true
      epsilon: 1e-8
      momentum: 0.999
      reset_stats_on_episode: false
    normalize_rewards:
      enabled: true
    frame_stack:
      enabled: false
      frame_stack: 1

  
# Evaluation defaults
evaluation:
  eval_episodes: 10

experiment:
  num_runs: 10