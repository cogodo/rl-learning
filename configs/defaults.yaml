# Meta-config: defines what algos, what envs, and overrides
algorithms:
  - dqn
  - sarsa
  # Add more algorithms as needed

environments:
  - CartPole-v1
  - MountainCar-v0
  # Add more environments as needed

# Global overrides
global:
  seed: 41
  max_episodes: 1000
  log_interval: 10
  
# Experiment settings
experiment:
  num_runs: 5
  save_checkpoints: true
  save_figures: true 

# Figure settings
figures:
  # Learning curves - most important for RL
  learning_curves:
    enabled: true
    metrics: ["episode_reward", "episode_length", "loss"]
    smoothing_window: 10
    show_std: true
    
  # Performance comparison across algorithms
  algorithm_comparison:
    enabled: true
    metrics: ["final_performance", "convergence_speed"]
    confidence_interval: 0.95
    
  # Hyperparameter sensitivity (for sweeps)
  hyperparameter_analysis:
    enabled: true
    parameters: ["learning_rate", "epsilon", "gamma"]
    
  # Training stability
  training_stability:
    enabled: true
    metrics: ["reward_variance", "loss_variance"]
    
  # Environment-specific visualizations
  environment_plots:
    enabled: true
    cartpole: ["pole_angle", "cart_position"]
    mountaincar: ["car_position", "car_velocity"]
    
  # Figure export settings
  export:
    format: "png"
    dpi: 300
    save_path: "results/figures/"

# global env settings
environment: 
  render_mode: null
  auto_reset: true

  #wrapper config defaults
  wrappers:
    normalize_observations: false
    normalize_rewards: false
    clip_actions: false
    frame_stack: 1
  
  #eval defaults
  eval_episodes: 10